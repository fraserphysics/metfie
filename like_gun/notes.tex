\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,afterpage}
%\usepackage{showlabels}
\usepackage[pdftex]{graphicx,color}
\usepackage{listings}
\newcommand{\normal}[2]{{\cal N}(#1,#2)}
\newcommand{\normalexp}[3]{ -\frac{1}{2}
      (#1 - #2)^T #3^{-1} (#1 - #2) }
\newcommand{\La}{{\cal L}}
\newcommand{\fnom}{\tilde f}
\newcommand{\fhat}{\hat f}
\newcommand{\COST}{\cal C}
\newcommand{\LL}{{\cal L}}
\newcommand{\Prob}{\text{Prob}}
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand\REAL{\field{R}}
\newcommand\Z{\field{Z}}
\newcommand{\partialfixed}[3]{\left. \frac{\partial #1}{\partial
      #2}\right|_#3}
\newcommand{\partiald}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\argmin}{\operatorname*{argmin}}
\newcommand{\argmax}{\operatorname*{argmax}}
\newcommand\norm[1]{\left|#1\right|}
\newcommand\bv{\mathbf{v}}
\newcommand\bt{\mathbf{t}}
\newcommand\Vfunc{\mathbb{V}}
\newcommand\Vt{\mathbf{V}}
\newcommand\vexp{V_{\rm{exp}}}
\newcommand\texp{T_{\rm{exp}}}
\newcommand\cf{c_f}
\newcommand\cv{c_v}
\newcommand\fbasis{b_f}
\newcommand\vbasis{b_v}
\newcommand\tsim{{\mathbf t}_{\rm{sim}}}
\newcommand\DVDf{\partiald{\Vt}{f}}
\newcommand\Lbb{\mathbb{L}}
\newcommand\epv{\epsilon_v}
\newcommand\epf{\epsilon_f}
\newcommand{\fig}[1]{figs/#1.pdf}

\title{Notes on EOS Estimation Code}

\author{Andrew M.\ Fraser}
\begin{document}
\maketitle
\begin{abstract}
  While source for these notes reside in
  metfie/like\_gun/notes.tex\footnote{Run \emph{git clone
      https://github.com/fraserphysics/metfie} to obtain the source.}
    and the illustrations are in the same directory, I use the same
    ideas for work on the 9501 model on moonlight.
\end{abstract}

\section{Notation for and Implementation of Basic Functions}
\label{sec:basic}

Given an experimental sequence of measured pairs of velocity and time
values, $(\vexp, \texp)$ and an initial model, $\fnom$, I describe how
to estimate a new model, $\fhat$.

\subsection{Notation}
\label{sec:basic_notation}

\begin{description}
\item[$(\vexp, \texp)$:] An experimental sequence of measured pairs of
  velocity and time
\item[$\bv$:] A sequence of model velocities
\item[$\Vt$:] A map from times to model velocities, eg, $\bv =
  \Vt(\texp)$
\item[$\tsim$:] A sequence of closely spaced sample times at which to record
  simulated position and velocity for constructing $\Vt$.
\item[$f$:] An EOS function
\item[$\Vfunc$:] An expensive procedure that maps an EOS function to a
  function that maps time to velocity with $\Vt = \Vfunc(f)$
\item[$\cv,\vbasis$:] Vectors of spline coefficients and basis functions
  that define a $\Vt$
\item[$\cf,\fbasis$:] Vector of spline coefficients and basis functions
  that define an EOS
\item[$D$] Matrix with elements $D[k,i] = \partiald{\cv[k]}{\cf[i]}$
\item[$B$] Matrix with elements $B[j,k] = \vbasis[k](\texp[j])$, ie,
  the exactly linear map from $c_v$ to simulated data.  The matrix
  elements are the velocity basis functions evaluated at the
  experimental sample times.
\item[$\DVDf$:] The derivative of $\Vt$ with respect to $f$; expensive
  to evaluate.  $\DVDf (\texp)$ is represented by the matrix $BD$.
\item[$\epv$] The vector of differences between the measured
  velocities and the simulated velocities at the times of the
  measurements.
\item[$\epf$] The vector of differences between the original
  coefficients $c_f$ and the current estimate of the coefficients.
  measurements (see \eqref{eq:epf}).
\item[$i,j,k$:] Indices: $j$ for experimental sample times; $k$ for
  the velocity basis functions $\vbasis[k]$ and their coefficients
  $\cv[k]$; And $i$ for the EOS basis functions and their coefficients
  $\cf[i]$.
\end{description}

\subsection{Implementation}
\label{sec:basic_implementation}

\begin{description}
\item[$\Vfunc(f)$:] Run a simulation and record a sequence of
  velocities $\bv$ at times $\tsim$.  Then fit a
  spline to $(\bv, \bt)$ to obtain $\Vt\iff \{cv,\vbasis\}$.
\item[$\Vt$:] Call the spline evaluation method to get $\bv = \Vt(\texp)$
\item[$f$:] A spline with coefficients $\cf$ and basis functions
  $\fbasis$
\item[$\DVDf$:] Evaluate $\Vfunc(f)$ for $N(\cf)+1$ different sets of
  coefficients $\cf$, and use finite differences to get a matrix with
  elements $D[k,i] = \partiald{\cv[k]}{\cf[i]}$.  Then
  \begin{equation*}
    \partiald{\Vt(T)}{\cf[i]} = \sum_k D[k,i] \vbasis[k](T).
  \end{equation*}
\end{description}

\section{Priors, Likelihood and Optimization}
\label{sec:opt}

\subsection{Cubic Splines}
\label{sec:splines}

I use cubic splines to approximate both the force function $f$ and the
velocity function $\Vt$.  Cubic splines are piecewise cubic.  I
illustrate their basis functions for a grid of evenly spaced knots in
Fig.~\ref{fig:basis}.  Note that the second derivative is affine
between knots and that consequently if it is positive at the knots it
is positive between them.  Also if the second derivative is positive
over the domain and the first derivative is positive at the right hand
end point, the function is monotonic and convex over the entire
domain.  Finally, if the function is monotonic and positive a the
right hand end point it is positive over the entire domain.
\begin{figure}
  \centering
    \resizebox{\columnwidth}{!}{\includegraphics{basis.pdf}}  
    \caption{Cubic spline basis functions and their first and second
      derivatives. The dashed plots are for $f_5$ whose knot is
      $t_5=3$.  Note that $t_k=0$ for $f_0,~f_1,~f_2$ and $f_3$, and
      $t_4=2$.}
  \label{fig:basis}
\end{figure}

Figure~\ref{fig:basis} also illustrates the symmetry of the right and
left boundaries.  The knot location $t$ of each basis function is the
point on the left where the function departs from zero.  There are
four functions at $t=0$.  There are also four functions at $t=10$, but
since their coefficients are always zero, they are not degrees of
freedom in any fit.

\subsection{Constraints}
\label{sec:constraints}

I require that the function $f$ be everywhere convex, monotonically
decreasing and positive.  Since I am using cubic splines, the third
derivative is piecewise constant, and for the knot positions in
Fig.~\ref{fig:basis} the constraints are equivalent to the following:
\begin{description}
\item[Second derivative positive at each knot] The matrix
  \setcounter{MaxMatrixCols}{12}
  \begin{equation}
    \label{eq:d2}
    D_2 =
    \begin{bmatrix}
      \frac{3}{2} & -\frac{5}{2} & 1     & 0   & \cdots \\ \\
      0   & \frac{2}{3} & -\frac{7}{6} & \frac{1}{2} & 0 & \cdots \\ \\
      \vdots   & 0    & \frac{3}{4} & -\frac{7}{4} & 1 & 0 & \cdots \\ \\
      & \vdots    & 0   & 1 & -2 & 1 & 0 & \cdots \\ \\
       & & & 0 & 1 & -2 & 1 & 0  \\ \\
      & & & & 0 & 1 & -2 & 1 & 0  \\ \\
      &&&&& 0 & 1 & -\frac{7}{4} &\frac{3}{4} & 0 \\ \\
      &&&&&& 0 & \frac{1}{2} & -\frac{7}{6} & \frac{2}{3} & 0 \\ \\
      &&&&&&& 0 & 1 & -\frac{5}{2} & \frac{3}{2} \\ \\
    \end{bmatrix}
  \end{equation}
  is the map from coefficients to the second derivative at the knots
  in Fig.~\ref{fig:basis}.
\item[First derivative negative at last knot] The vector
  \begin{equation*}
    D_1 = \begin{bmatrix}
    0 & \cdots & 0 & -\frac{3}{2} & \frac{3}{2}
  \end{bmatrix}
  \end{equation*}
  is the map from coefficients to the first derivative at the last knot
  in Fig.~\ref{fig:basis}.
\item[Function value positive at last knot] The vector
  \begin{equation*}
     D_0 = \begin{bmatrix}
    0 & \cdots & 0 & 1
  \end{bmatrix}
  \end{equation*}
  is the map from coefficients to the function value at the last knot
  in Fig.~\ref{fig:basis}.
\end{description}
Thus for the knots in Fig.~\ref{fig:basis}, the constraints are
\begin{align}
  G & \equiv
      \begin{bmatrix}
        - D_2 \\ D_1 \\ -D_0
      \end{bmatrix}\\
  \label{eq:constraint}
  G\cf & \preceq 0.
\end{align}

\subsection{Minimum Squared Error}
\label{sec:minsq}

Before calculating the step for the full log a posteriori probability,
I calculate a least squares step (maximizing the likelihood if all
$\sigma_v[i]$ are equal to each other).  Thus, I wish to find $d$ that
minimizes
\begin{equation}
  \label{eq:ssq}
  \tilde S(d) = \sum_{j=1}^{N_{\rm exp}} (\vexp[j] - \Vt_{f+\delta}(\texp[j]))^2,
\end{equation}
where $f+\delta = \sum_i (\cf[i] + d[i])\fbasis[i]$, and 
\begin{equation}
  \label{eq:taylor}
  \Vt_{f+\delta}(\texp[j]) = \Vt_f(\texp[j]) +
  \sum_k \sum_i \partiald{\cv[k]}{\cf[i]}d[i]\vbasis[k](\texp[j])
  +\text{ HOT}.
\end{equation}
Defining
\begin{equation}
  \label{eq:epsilon}
  \epv[j] \equiv \vexp[j] - \Vt_f(\texp[j])
\end{equation}
and dropping HOT, I write
\begin{align}
  S(d) &\equiv \tilde S(d) - \text{HOT} = \sum_j \left( \epv[j] -
  \sum_i \sum_k \partiald{\cv[k]}{\cf[i]}d[i]\vbasis[k](\texp[j])
  \right)^2 \nonumber \\
  \label{eq:sd}
  &= (\epv - BDd)^T(\epv - BDd)\\
  &= d^TD^TB^TBDd - 2 \epv^TBDd + \epv^T\epv \nonumber
\end{align}
Thus I seek $\hat d$ that minimizes $S(d)$ subject to the constraint
\eqref{eq:constraint}.  The following manipulations put the constraint
in terms of $d$:
\begin{align*}
  \cf{_{_1}} &\equiv \cf{_{_0}} + d \\
  G \cf{_{_1}} &= G \left( \cf{_{_0}} + d \right) \preceq 0 \\
  G d &\preceq -G \cf{_{_0}} \equiv h
\end{align*}


This is the quadratic programming problem
\begin{subequations}
  \label{eq:dhat}
  \begin{align}
    \text{Minimize } & \frac{1}{2} d^T P d + q^T d \\
  \label{eq:dhatb}
    \text{Subject to } & Gd \preceq h
  \end{align}
\end{subequations}
with
\begin{align*}
  P &= D^TB^TBD \\
  q^T &= -\epv^TBD \\
  h &= -G\cf
\end{align*}
where I've put \eqref{eq:dhat} in a form that matches
\emph{cvxopt.solvers.qp}.

\subsection{A Posteriori Probability}
\label{sec:app}

I use Gaussians with constant diagonal covariances for priors and
likelihood, and I will iterate to maximize the a posteriori
probability starting from a prior model for the coefficients with a
mean $c_{\fnom}$ and variance $\Sigma_f$.  If after a number of
iterations the estimate of $f$ is given by $c_f$, I will consider
changes $\delta$ to $f$ implemented by changes $d$ to $c_f$.  The
prior distribution for the variable $d$ is
\begin{align}
  \cf+d &\sim \normal{\cf+d - c_{\fnom}}{\Sigma_f} \nonumber \\
  \label{eq:priord}
  d &\sim \normal{d - \epf}{\Sigma_f}
\end{align}
where
\begin{equation}
  \label{eq:epf}
  \epf \equiv c_{\fnom} - c_f,
\end{equation}
and its likelihood is
\begin{equation}
  \label{eq:liked}
  \vexp,\texp | d \sim
  \normal{\vexp-\Vt_{f+\delta(d)}(\texp)}{\Sigma_v}.
\end{equation}
From \eqref{eq:taylor} I use the first order (ie, affine)
approximation
\begin{equation*}
  \Vt_{f+\delta(d)}(\texp) \approx \epv - BDd,
\end{equation*}
and write the log probabilities:
\begin{align}
  L_c(\cf+d) & \equiv \log(K_2) + \log \left(\Prob(d) \right)\nonumber\\
  \label{eq:L_c}
  & = \log(K_2) + \normalexp{d}{\epf}{\Sigma_f}\\
  L_v(\Vt_{f+\delta}) & \equiv \log(K_1) + \log
  \left(\Prob(\vexp,\texp | f+\delta) \right) \nonumber\\
  \label{eq:L_v}
  &\approx \log(K_1) + \normalexp{BDd}{\epv}{\Sigma_v}.
\end{align}

Given $f$ (in terms of $\cf$) and the functions $\Vt$, $\DVDf$, etc.\
that depend on it, an optimization step consists of ignoring the
\emph{higher order terms} (HOT) in $\Vt_{f+\delta}$ and solving for
the vector $d$ that maximizes
\begin{align*}
  \Lbb(\cf,d) &= L_c(\cf + d) + L_v(\Vt_{f+\delta}(\texp)) \\
  & \equiv -\frac{1}{2} d^T P d - q^T d -
    \frac{1}{2} R
\end{align*}
subject to the constraints \eqref{eq:dhatb}.  Manipulation of
\eqref{eq:L_c} and \eqref{eq:L_v} yields
\label{eq:PqR}
\begin{subequations}
  \begin{align}
    P &= \Sigma_f^{-1} + (BD)^T \Sigma_v^{-1} BD \\
    q^T &= \epf^T \Sigma_f^{-1} - \epv^T
          \Sigma_v^{-1} BD  \\
    \label{eq:R}
    R &= \epf^T \Sigma_f^{-1} \epf + \epv^T \Sigma_v^{-1} \epv.
  \end{align}
\end{subequations}
\subsubsection{Preconditioning}
\label{sec:preconditioning}

If the interval explored by the experiment is inside the domain of $f$
in $X$, then $B$ will be singular.  Since the variance of the prior at
$x$ is proportional $(f(x))^2$ and $f(x) \approx \frac{C}{x^3}$,
\begin{equation}
  \label{eq:var_f}
  \sigma^2_{c_{f,i}} \propto x_i^{-6}.
\end{equation}
Thus the condition of $P$ (the ratio of the largest eigenvalue to the
smallest eigenvalue) is bad.  By changing to the variable $z$ defined
by
\begin{align*}
  z_i &\equiv x_i^3 d_i \\
  z &\equiv U \cdot d \text{ with} \\
  U_{i,j} &= \delta_{i,j} x_i^3,
\end{align*}
\newcommand{\UI}{U^{-1}} instead of $P$, the optimization code will use
$\UI P \UI$, which is better conditioned.  I also rescale the
constraint equation to ensure that $\left| h_i \right|=1~\forall i$
using the matrix
\begin{equation*}
  H^{-1}_{i,j} \equiv \delta_{i,j} \frac{1}{\left|h_i\right|}
\end{equation*}


The procudure is to first solve the quadratic program specified by
\begin{align*}
  \tilde P &= \UI P \UI \\
  \tilde q &= \UI q \\
  \tilde G &= H^{-1}G \UI \\
  \tilde h &= H^{-1}h
\end{align*}
for $z$, and then derive the estimate of $d$ from
\begin{equation*}
  d = \UI z.
\end{equation*}



Aaron suggested ``Stability of Symmetric Ill-Conditioned Systems
Arising in Interior Methods for Constrained Optimization'' by Anders
Forsgren, Philip E. Gill, and Joseph R. Shinnerl in SIAM. J. Matrix
Anal. \& Appl., 17(1),
187â€“211. http://epubs.siam.org/doi/abs/10.1137/S0895479894270658

\subsection{The Code \emph{calc.py}}
\label{sec:code}

In the following methods of class \emph{GUN} which implement the
optimization, the values of the last 4 spline coefficients of $f$ are
not varied because they are fixed at 0.
\begin{description}
\item[set\_t2v] Run a simulation to build a spline for mapping times to
  velocities and save as self.$t2v$.  The result for the initial EOS
  appears in Fig.~\ref{fig:vt_gun}.
  \begin{figure}
    \centering
    \resizebox{\columnwidth}{!}{\includegraphics{\fig{vt_gun}}}
    \caption{Velocity as functions of time.  Traces for the
      \emph{experimental} data, a simulation with the initial EOS and
      the error.}
    \label{fig:vt_gun}
  \end{figure}
\item[set\_D] Use finite differences to calculate dv/df in terms of
  spline coefficients and save as self.$D$ with
  $D[k,i] = \frac{\Delta c_v[k]}{\Delta c_f[i]}$.  See
  Fig.~\ref{fig:D_gun} and note that $k$ and $i$ range over variable
  components of $\cv$ and $\cf$.
  \begin{figure}
    \centering
    \resizebox{\columnwidth}{!}{\includegraphics{\fig{D_gun}}}
    \caption{The matrix $D$.  Each trace represents
      $\frac{\partial \cv[k]}{\partial \cf[i]}$ for a different $i$.}
    \label{fig:D_gun}
  \end{figure}
\item[set\_Be] Map experimental velocities v and times t to the
  following:
  \begin{align*}        
    \text{self.}ep[i] &= v[j] - t2v(t[j])
                        \text{ Difference between simulation and data} \\
    \text{self.}B[j,k] &= \vbasis[k](t[j])     
  \end{align*}
  Notes: $\vbasis[k]$ is the kth basis function for the t2v spline.
  $B$ is the derivative of the simulated velocity function at the
  experimental sample times wrt the coefficients of the spline fit
  that implements that function, and $BD_{j,i}$ is the derivative of
  $v(t[j])$ wrt $\cf[i]$.  See Fig.~\ref{fig:BD_gun}.
  \begin{figure}
    \centering
    \resizebox{\columnwidth}{!}{\includegraphics{\fig{BD_gun}}}
    \caption{The product $B D$.  Each trace represents
      $\frac{\partial v(t)}{\partial c_f[i]}$ for a different $i$.
      Note that each trace is a difference between splines fit to
      simulations with a finite difference in $f$.  Because the basis
      functions for $v(t)$ are very narrow, this figure is very
      similar to Fig.~\ref{fig:D_gun}.}
    \label{fig:BD_gun}
  \end{figure}
\item[func] From \eqref{eq:sd} the objective function is:
  \begin{equation*}
    S(d) = \left| \epv - BD\cdot d \right|^2.
  \end{equation*}
  The difference between the simulated and measured velocities is
  $\epv$, and $d$ is the change in the vector of spline
  coefficients.  $BD$ is the derivative of the simulated velocities
  wrt to $d$.  Note that $d\in \REAL^{n_c}$ where $n_c = \text{len}(c_f)-4$.
\item[opt] Do a constrained optimization step.  The goal of \emph{opt}
  is to find the change $d$ in spline coefficients $c_f$ that yields a
  function $f(c_f+d)$ that minimizes $F(f)$.  The result appears in
  Fig.~\ref{fig:opt_result}.
  \begin{figure}
    \centering
    \resizebox{\columnwidth}{!}{\includegraphics{\fig{opt_result}}}
    \caption{The function $f_1 = f_0 + \hat d$ returned by fit.opt()
      compared to $f_0$.}
    \label{fig:opt_result}
  \end{figure}
\end{description}

\subsection{Numerical Results}
\label{sec:numerical-results}

\newcommand{\freq}{k} %
Figures \ref{fig:big_d} and \ref{fig:fve_gun} illustrate iterative maximum
likelihood fitting of an EOS.  The procedure starts with the following
initial function
\begin{equation}
  \label{eq:initial}
  \fnom(x) = \frac{C}{x^3},
\end{equation}
and the iterations approach the \emph{actual} function
\begin{align}
  \label{eq:actual}
  f(x) &= \fnom(x) + \frac{2 e^{-\frac{x^2}{2w^2}}
         \fnom(x_0)}{\freq}  \sin(\freq x)\\
   \text{ where }C &= 2.56\times 10^{10} \nonumber \\
  \freq &= 0.2 \nonumber \\
  w &= 0.2, \nonumber
\end{align}
which was used to generate the simulated data.
\begin{figure}
  \centering
    \resizebox{\columnwidth}{!}{\includegraphics{\fig{big_d}}}
    \caption{Illustration of the derivative of the velocity function
      with respect to the pressure function, $\frac{d v}{d p}$.  The
      second and third columns correspond variations of the
      coefficients of basis functions that differ by a factor of ten.
      The lower left plot indicates that nonlinear part of the
      response is a factor of a $10^4$ smaller than the linear
      response.  }
  \label{fig:big_d}
\end{figure}
\begin{figure}
  \centering
    \resizebox{\columnwidth}{!}{\includegraphics{\fig{fve_gun}}}
    \caption{Maximum likelihood estimation of $f$.  The \emph{true}
      EOS appears as \emph{actual} in the upper plot, and the
      optimization starts with the \emph{initial} and ends with
      \emph{fit}.  The corresponding velocity as a function of
      position appears in the middle plot, and the sequence of errors
      in the velocity time series for each step in the optimization
      appears in the lower plot. }
  \label{fig:fve_gun}
\end{figure}

\section{Comparing Model Classes}
\label{sec:classes}

We want to test the hypothesis that splines are better for building
models of an EOS than polynomials.  We hope to compare the bias and
variance of two sequences of estimators, one that uses splines and one
that uses polynomials.  We will study estimators for quantities of
interest that are functionals of the EOS that differ from the
functionals that correspond to the measurements.  In previous work, we
have analyzed the functionals for muzzle time and muzzle velocity for
an ideal gun, and in the sections above we have described an analysis
of velocity measurements that depend on time.  We hope that those
quantities of interest and those measurements are sufficiently
different in that the measurements are given as functions of time and
the quantities of interest are required as functions of position.

Here is the agenda:
\begin{enumerate}
\item Design two classes of EOS models, ie, pressure as a function of
  volume along an isentrope.  One class will be splines with $n$ knots
  equally spaced on a log scale, and the other will be $n^{\text{th}}$
  order polynomials.
\item Write code that draws realizations of experimental data based on
  an EOS that is outside of both model classes for any finite $n$.
\item Write code that estimates the coefficients of polynomial models
  of the EOS.
\item Calculate (or estimate with simulations) sequences (indexed by
  $n$) of bias and variance pairs of estimators for quantities of
  interest using the two classes of models.  We hope to find that the
  curve defined by the pairs for splines lies below the curve defined
  by pairs for polynomials.
\end{enumerate}

\subsection{Alternative Measurements}
\label{sec:alternative}

If we need measurements that are more different from the quantities of
interest, we have could use the following simplification of
measurements of cylinders.  Suppose that all of the material in a long
cylinder detonates in an instant and that the density after that is
uniform inside.  Thus the function $\left\{r(t): t>0 \right\}$ is a
sufficient specification of an experimental result.  Letting
$r(0) = r_0$ denote the initial radius the work done by the gas in
expanding to $r$ is
\begin{equation*}
  U(r) = \int_{r_0}^r p(v(r)) \frac{dv}{dr} dr = \int_{r_0}^r p(\pi
  r^2) 2\pi r dr.
\end{equation*}
That work is used to deform and accelerate the cladding.  We say that
the energy of deformation is
\begin{equation*}
  D(r) = \sigma \log \left(\frac{r}{r_0} \right),
\end{equation*}
and the kinetic energy of the cladding is
\begin{equation*}
  K = \frac{1}{2} m \left( \dot r \right)^2.
\end{equation*}
Thus the velocity as a function of radius is
\begin{equation*}
  v(r) = \sqrt{\frac{2}{m} \left( U(r) - \sigma \log
      \left(\frac{r}{r_0} \right) \right) },
\end{equation*}
and we can obtain the velocity as a function of time by numerical
integration.

\section*{Appendix}
\label{sec:appendix}

See other information in cmfSuite/doc/HE.tex on moonlight.

%
\vfill \hrule

Source file: https://github.com/fraserphysics/metfie/like\_gun/notes.tex

\end{document}

%%%---------------
%%% Local Variables:
%%% eval: (TeX-PDF-mode)
%%% eval: (setq ispell-personal-dictionary "./localdict")
%%% End:
