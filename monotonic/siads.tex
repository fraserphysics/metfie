\documentclass[12pt]{article} \usepackage{amsmath,amsfonts}
%\usepackage{showlabels}
\usepackage[pdftex]{graphicx,color}

\newcommand{\htop}{{h_{\text{top}}}}
\newcommand{\T}{{\cal P}}
\newcommand{\logprob}{{\cal L}}

\title{Uniform Probability for Subsequences Drawn from a Continuous
  Time Monotonic Process }

\author{Andrew M.\ Fraser}
\begin{document}
\maketitle
\begin{abstract}
  I describe a probability measure for continuous time trajectories that
  are constrained both to lie in within certain bounds and to be
  monotonic.  Roughly, the measure makes all allowed sequences have
  the same probability with respect to a norm.
\end{abstract}

\section*{To Do:}
\label{sec:do}

\begin{itemize}
\item Pictures of allowed pairs
\item Code that calculates for given $\Delta$:
  \begin{itemize}
  \item Stationary distribution
  \item Conditional distribution ofter $n$ steps or after $\Delta t$
  \end{itemize}

\end{itemize}
\begin{figure*}
  \centering
  \resizebox{0.75\textwidth}{!}{\includegraphics{bounds.pdf}}
  \caption{Bounds on allowed functions}
  \label{fig:mt2}
\end{figure*}

\section{Introduction}
\label{sec:introduction}
\begin{description}
\item[Motivation:] Priors for regions in function space constrained by
  physical laws.  Cite Hixson2000, Fritz1996,
  Pemberton2011LA-UR-11-04999, and FraserLA-UR-13-21824
\item[Connection to Information Theory:] Theorem 8 of Shannon.  Define
  stationary and Markov for discrete time discrete value process.
\item[Connection to Statistical Mechanics:] Jaynes maximum entropy
  derivation of Maxwell Boltzmann distribution.  Coordinate dependent.
\item[Connection to path integrals:]
\item[Not Levy process:] Connection to Brownian motion
\item[Outline:]
\end{description}

\section{A simple example}
\label{sec:example}

Consider the adjacency matrix
\begin{equation}
  \label{eq:A}
  A = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix},
\end{equation}
the schematic of the corresponding finite state Markov process in
Figure~\ref{fig:mt2}, and its probability matrix
\begin{equation}
  \label{eq:T}
  \T = \begin{bmatrix} a & b \\ c & 0 \end{bmatrix}.
\end{equation}

\begin{figure*}
  \centering
  \resizebox{0.4\textwidth}{!}{\input{mt2.pdf_t} }
  \caption{A schematic of the finite state Markov process specified by
  Equation~\eqref{eq:T}.}
  \label{fig:mt2}
\end{figure*}

Given values for $a$, $b$, and $c$, one can solve
\begin{equation}
  \label{eq:stationary}
  \mu = \mu \T
\end{equation}
for the stationary distribution $\mu$ and from there find the entropy
rate $h$ by
\begin{equation}
  \label{eq:rate}
  h = \sum_i \mu_i H(J|i)
\end{equation}
where
\begin{equation*}
  H(J|i) \equiv -\sum_j \T_{i,j} \log (\T_{i,j})
\end{equation*}
is the familiar $P\log(P)$ formula.

\subsection{Parameters of $\T$ that maximize $h$}
\label{sec:max}

For the maximum entropy values, given a trajectory length $N$, all
trajectories must have almost the same probability.  Since all
trajectories are composed of loops in a cycle basis, the following
equation must hold
\begin{equation}
  \label{eq:excycle}
  a^2 = b\cdot c.
\end{equation}
That combined with the normalization constraints for the two nodes
\begin{align*}
  a + b &= 1 \\
  c &= 1
\end{align*}
is sufficient to specify the solution
\begin{align*}
  a &= \frac{2}{1+\sqrt{5}} \approx          0.61803\\
  b &= \frac{\sqrt{5}-1}{1+\sqrt{5}} \approx 0.38197\\
  c &= 1.
\end{align*}
Solving \eqref{eq:stationary} for that specification of $\T$ yields
\begin{align*}
  \mu &=  \begin{bmatrix} \frac{1+\sqrt{5}}{2\sqrt{5}}, &
    \frac{\sqrt{5}-1}{2\sqrt{5}} \end{bmatrix} \\
  h &= \log\left(\frac{1+\sqrt{5}}{2}\right).
\end{align*}


\subsection{Topological entropy $\htop$}
\label{sec:htop}

The topological entropy of a directed graph specified by an adjacency
matrix $A$ is the rate at which the number of trajectories grows with
length.  If $n(t)\equiv\begin{bmatrix} n_1(t),&n_2(t)\end{bmatrix}$
denotes the number of trajectories of length $t$ that end in the two
states of Figure \ref{fig:mt2}, then
\begin{align*}
  n(1) &= \begin{bmatrix} 1,&1\end{bmatrix} \text{ and} \\
  n(t+1) &= n(t) A.
\end{align*}
Thus
\begin{align*}
  n(t) &= \begin{bmatrix} 1,&1\end{bmatrix} A^t \\
  \text{and }\lim_{t\rightarrow \infty} \frac{1}{t} \log n_1(t) &=
  \lambda
\end{align*}
where $\lambda$ is the largest eigenvalue of $A$.

Solving the eigenvalue problem yields
\begin{equation*}
  \htop = \log\left(\frac{1+\sqrt{5}}{2}\right).
\end{equation*}

The topological entropy provides one more equation of constraint on
the equations that the values in $\T$ must satisfy for maximizing $h$.
Rather than the single equation \eqref{eq:excycle}, the pair
\begin{equation*}
  - \htop = 2\log(a) = \log(b) + \log(c)
\end{equation*}
must hold.

\section{A $4\times 4$ example}
\label{sec:fourbyfour}

For the adjacency matrix
\begin{equation*}
  A =
  \begin{bmatrix}
        1 & 1 & 0 & 1 \\
        0 & 1 & 1 & 1 \\
        1 & 0 & 1 & 1 \\
        0 & 1 & 1 & 0 \\
  \end{bmatrix}
\end{equation*}
I seek the transition matrix
\begin{equation*}
  \T =
  \begin{bmatrix}
        a & b & 0 & c \\
        0 & d & e & f \\
        g & 0 & h & i \\
        0 & j & k & 0 \\
  \end{bmatrix}
\end{equation*}
that maximizes the entropy rate.  Building on \emph{networkx}, I find
the following cycle basis where elements are characterized by a list
of edges and a list of signs that indicate the direction that each
edge is traversed.
\begin{align*}
  C_1 &=
  \begin{bmatrix}
    (3,2) & (2,1) & (1,0) & (0,3) \\
    1 & -1 & -1 & 1
  \end{bmatrix} \\
  C_2 &=  \begin{bmatrix} (0,0) \\ 1 \end{bmatrix} \\
  C_3 &=  \begin{bmatrix} (1,1) \\ 1 \end{bmatrix} \\
  C_4 &=  \begin{bmatrix} (2,2) \\ 1 \end{bmatrix} \\
  C_5 &= \begin{bmatrix}
    (3,1) & (1,0) & (0,3) \\
    1 & -1 & 1  \end{bmatrix} \\
  C_6 &= \begin{bmatrix}
    (2,3) & (3,0) & (0,1) & (1,2) \\
    1 & -1 & 1 & 1
  \end{bmatrix} \\
  C_7 &= \begin{bmatrix}
    (1,3) & (3,0) & (0,1) \\
    1 & -1 & 1  \end{bmatrix} \\
  C_8 &= \begin{bmatrix}
    (2,0) & (0,1) & (1,2) \\
    1 & 1 & 1  \end{bmatrix} \\
\end{align*}

Using the abbreviation $\logprob_{i,j} = -\log(\T_{i,j})$, I write the eight
loop equations as
\begin{align*}
  0 &= \logprob_{3,2} - \logprob_{2,1} - \logprob_{1,0} + \logprob_{0,3} \\
  H &= \logprob_{0,0} \\
  H &= \logprob_{1,1} \\
  H &= \logprob_{2,2} \\
  H &= \logprob_{3,1} - \logprob_{1,0} + \logprob_{0,3} \\
  2H &= \logprob_{2,3} - \logprob_{3,0} + \logprob_{0,1} + \logprob_{1,2} \\
  H &= \logprob_{1,3} - \logprob_{3,0} + \logprob_{0,1} \\
  3H &= \logprob_{2,0} + \logprob_{0,1} + \logprob_{1,2},
\end{align*}
where $H$ is the unknown entropy.  With the four node equations
each of which has the form
\begin{equation*}
  \sum_j \T_{i,j} = 1,
\end{equation*}
I have 12 equations for 12 unknowns.  I have not found a method to
solve such systems in general.

\section{A Power Iteration Approach}
\label{sec:algorithm}

I will use the solution to this problem to characterize the
distribution of long trajectories in a system defined by an adjacency
matrix $A$.  I want the distribution to be uniform over all allowed
trajectories.

Given a list of allowed trajectories of length $2T$, I could
approximate the value of $\T_{i,j}$ by
\begin{equation*}
  \hat \T_{i,j} = \frac{_{2T}n_{T,T+1}(i,j)}{_{2T}n_{T}(i)},
\end{equation*}
where $_{2T}n_{T}(i)$ denotes the number of trajectories of length
$2T$ for which $i$ is the value at position $T$ and similarly
$_{2T}n_{T,T+1}(i,j)$ denotes the number of trajectories that have
values $i$ and $j$ at positions $T$ and $T+1$ respectively.  While the
exponential growth of the number of trajectories suggests that
implementing the estimate for large $T$ is not feasible, power
iterations for left and right eigenvectors of $A$ make it easy.

The number of allowed sequences of length say six that exactly matches
a given sequence is either zero or one and can be written as
\begin{equation*}
  _6n_{1,2,3,4,5,6}(a,b,c,d,e,f) = A_{a,b}A_{b,c}A_{c,d}A_{d,e}A_{e,f}.
\end{equation*}
The number of sequences that are only required to match at positions
three and four is
\begin{equation*}
 _6n_{3,4}(c,d) =  \sum_{a,b,e,f} {_6n}_{1,2,3,4,5,6}(a,b,c,d,e,f) =
 \sum_{a,f} \left(A^2\right)_{a,c} A_{c,d} \left(A^2\right)_{d,f}.
\end{equation*}
Similarly
\begin{equation*}
 _{202}n_{101,102}(c,d) = \sum_{a,f} \left(A^{100}\right)_{a,c} A_{c,d} \left(A^{100}\right)_{d,f}.
\end{equation*}

I define (and calculate) $R(t)$ and $L(t)$ recursively with the
following power iteration scheme
\begin{align*}
  L(1) &= \begin{bmatrix} 1,&1,&\cdots,&1,&1\end{bmatrix} \\
  N_L(t) &= \left| L(t) \right| \\
  L(t+1) &= \frac{L(t) A}{N_L(t)} \\
  R(1) &= L^{\text{T}}(1) \\
  N_R(t) &= \left| R(t) \right| \\
  R(t+1) &= \frac{A R(t)}{N_R(t)}.
\end{align*}
Note that as $t$ increases, $R(t)$ and $L(t)$ converge quickly to the
right and left eigenvectors respectively of $A$ that correspond to the
largest eigenvalue and I can calculate
\begin{equation}
  \label{eq:N}
   _{2(T+1)}n_{T+1,T+2}(c,d) \propto \left(L(T)\right)_c  A_{c,d}
   \left(R(T)\right)_d
\end{equation}
pretty easily.  With results from \eqref{eq:N} for a large enough $T$
to ensure convergence, one can calculate estimates $\hat \mu$ and
$\hat \T$ of the stationary distribution and transition probabilities
respectively as follows.
\begin{align*}
  \tilde L &= L(T) \\
  \tilde R &= R(T) \\
  \tilde P_{i,j} &= \tilde L_i A_{i,j} \tilde R_j \\
  \tilde m_i &= \sum_j \tilde P_{i,j} \\
  \hat \mu &= \frac{\tilde m}{\sum_i \tilde m_i} \\
  \hat \T_{i,j} &= \frac{\tilde P_{i,j}}{\tilde m_i}
\end{align*}

Applying the algorithm to the example of Section \ref{sec:fourbyfour}
yields
\newcommand{\first}{\frac{\sqrt{3}-1}{2}}
\newcommand{\second}{2-\sqrt{3}}
\begin{align*}
  \hat \mu &=
  \begin{bmatrix}
     0.18795109  & 0.27517816 & 0.32554595 & 0.21132481
  \end{bmatrix} \\ \\
  \hat \T &=
  \begin{bmatrix}
 0.36602534 & 0.36602534 & 0. &         0.26794932  \\
  0. &         0.36602534 & 0.36602534 & 0.26794932 \\
  0.36602534 & 0. &         0.36602534 & 0.26794932 \\
  0. &         0.5 &        0.5 &        0.         
  \end{bmatrix} \\ \\
  &=
  \begin{bmatrix}
    \first & \first & 0      & \second \medskip \\
    0      & \first & \first & \second \medskip \\
    \first & 0      & \first & \second \medskip \\
    0      & \frac{1}{2} & \frac{1}{2} & 0
  \end{bmatrix} \\
  h &= 1.005053
\end{align*}

Maxentropic Markov chains appears in:
IEEE Transactions on Information Theory, 
Date of Publication: Jul 1984
Author(s): Justesen, J.
Hoholdt, T.
Volume: 30 , Issue: 4
Page(s): 665 - 667

\section{Composition}
\label{sec:composition}

\begin{align*}
  \left( P(0\mapsto 1) \right)_{i,j} &= \frac{L_i A_{i,j} R_j}
    {\lambda z \frac{\sum_k L_i A_{i,k} R_k}{\lambda z}} \\
  &= \frac{A_{i,j} R_j}{\lambda R_i} \\
  \left( P(0\mapsto 2) \right)_{i,j} &=
     \sum_k \frac{A_{i,k} R_k}{\lambda R_i}
            \frac{A_{k,j} R_j}{\lambda R_k} \\
  &= \frac{(A^2)_{i,j} R_j}{\lambda^2 R_i}                                       
\end{align*}
If $B$ is the adjacency matrix for twice the step size of $A$,
sufficient conditions for consistency are
\begin{align*}
  B R &= \gamma R \\
  \frac{B_{i,j} R_j}{\gamma R_i} &= \frac{(A^2)_{i,j} R_j}{\lambda^2
                                   R_i} \text{or}\\
  (A^2)_{i,j} &= \frac{\lambda^2}{\gamma} B_{i,j}.
\end{align*}
The conditions are not plausible because if it is possible to get from
$i$ to $j$ in two steps, ie, $B_{i,j}=1$, the number of ways to get
from $i$ to $j$ in two steps will vary depending on the values of $i$
and $j$.

\end{document}

%%%---------------
%%% Local Variables:
%%% eval: (TeX-PDF-mode)
%%% End:
